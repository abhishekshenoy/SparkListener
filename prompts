This is great , now I want to achieve one more thing. 

I have a use case wherein I am trying to capture metadata around all the workflows in my ETL workflows and I want to write a pluggable listener to my Apache Spark ETL jobs. 

My goal that I want to achieve with this listener is as below .

Whenever there is an Action in called in Spark , I want my listener to capture the below details. 

1) I want to capture the different data sources that are read and currently want to focus only on the below sources. 
* It can be a file source on local filesystem , cloud storage or Hadoop file system 
* It can be a table in hive which is again data in any of the file systems. 

2) Metrics around Files Read , Records Read , Data Size Scanned from the source. 

3) The filter operations that are applied on the above sources.

4) Metrics post the filter operations are applied. 

Once you are able to capture the metrics what I want to represent is below. 

For each action capture SourceNode Details and Target Node Details.

1 Action there can be N Source Node Details but only 1 Target Node Detail.

In Source Node Detail I want to capture Table/Folder Name , columns projected , filters applied , Metrics before the filter , Metrics After Filter

In Target Node Detail I want to capture Table/Folder Name and Metrics.

I want to have the Listner written in Scala . Also to give a hint , I have tried this multiple times using other AI but all are able to capture the Metadata around tables and filters but none were able to give the correct code to accurate capture the metrics and represent it as I have mentioned. Hope you do a deep analysis of the Spark Source Code and help me figure this out correctly.


——————————————————————————————————————————————————————————————————————————————————————————————————————

This is not testable , why are the functions kept empty ? 

Consider that the the logic is only meant for FileSources , ignore the the Hive Table logic for now. As per my knowledge , this logic should be same and deducible irrespective of type of FileSource (LocalFs , HDFS , Cloud Storage). Also Let us bring another level of clarity , consider this only for File Types of Parquet and ORC , ignore other FileTypes.

And also for Write Action consider only the Sink as any File Sink (LocalFileSystem , CloudStorage , HDFS) . As per my knowledge as long as the Sink is of FileType , should have the same command. 

```
  private def extractFiltersFromRelation(relation: HiveTableRelation): List[String] = {
    // This would need to be implemented based on how filters are pushed down
    // For now, returning empty list
    List.empty
  }

  private def extractFiltersFromLogicalRelation(relation: LogicalRelation): List[String] = {
    // Extract filters from the relation if available
    List.empty
  }

private def getFileCount(locationUri: Option[java.net.URI]): Long = {
    // Implementation to count files in the location
    // This is a placeholder - actual implementation would depend on your file system
    1L
  }

```


One more issue that I am seeing is during compile time I am getting this error for the below code :

 ';' expected but '=>' found. 

```
private var metadataCallback: ActionMetadata => Unit = _ => {} 
```

Can you please help me with the above and regenerate the code ?


——————————————————————————————————————————————————————————————————————————————————————————————————————


I do not want to have this additional call from my running code .

metadataListener.captureQueryExecution(spark.sparkContext.getNextJobId, filterQuery)

 I want everything to be abstracted out by the listener itself , that is all the downstream user needs to do is add this in his logic and things should be handled.

spark.sparkContext.addSparkListener(metadataListener)

——————————————————————————————————————————————————————————————————————————————————————————————————————